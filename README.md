# SaaS GTM Revenue & Ops Control Tower

End-to-end analytics stack demonstrating modern data engineering and analytics best practices

A complete, production-ready analytics platform for B2B SaaS go-to-market organizations. This project showcases synthetic data generation, warehouse modeling with dbt, comprehensive data quality testing, and stakeholder-ready dashboardsâ€”all running locally via Docker.

---

## Overview

This control tower provides unified visibility into:
- **Revenue Metrics**: MRR, ARR, NRR, expansion, contraction, churn
- **Pipeline Performance**: Win rates, sales cycle, conversion by segment
- **Product Adoption**: Activation rates, time-to-value, usage patterns
- **Customer Retention**: Cohort analysis, churn drivers, lifetime value
- **Support Quality**: Ticket volume, SLA compliance, resolution times
- **Marketing Efficiency**: CAC by channel, payback periods, ROAS
- **Anomaly Detection**: Automated flagging of significant metric shifts

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Generator â”‚  â† Python script (Faker, pandas, numpy)
â”‚  (Synthetic)    â”‚     18 months of realistic SaaS data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ CSVs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Postgres DB   â”‚  â† Warehouse (Docker)
â”‚   (raw_* tables)â”‚     9 raw tables loaded via Python
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Project   â”‚  â† Transformations (staging â†’ marts)
â”‚                 â”‚     9 staging views
â”‚                 â”‚     2 dimension tables
â”‚                 â”‚     7 fact tables
â”‚                 â”‚     20+ schema tests
â”‚                 â”‚     2 custom SQL tests
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit App  â”‚  â† Dashboard (5 pages)
â”‚  (Analytics UI) â”‚     Executive Overview
â”‚                 â”‚     Funnel & Pipeline
â”‚                 â”‚     Retention & Cohorts
â”‚                 â”‚     Support & Quality
â”‚                 â”‚     Anomalies
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dashboard Demo

Watch the complete 5-page interactive dashboard walkthrough:

**[ðŸ“º View Dashboard Demo Video](https://github.com/saitejareddy1220/SaaS-GTM-Revenue-Ops-Control-Tower/releases/download/v1.0/dashboard.mp4)**

**Dashboard Pages:**
- Executive Overview with KPIs (MRR, ARR, CAC, Activation Rate)
- Pipeline Analytics (Win rates by segment, sales cycle)  
- Cohort Retention Heatmaps (12-month retention tracking)
- Support Metrics (Ticket volume, SLA compliance)
- Anomaly Detection (Automated spike/drop identification)

---

## Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Orchestration** | Docker Compose, Makefile | Single-command deployment |
| **Warehouse** | PostgreSQL 15 | Data storage and query engine |
| **Transformation** | dbt 1.10+ | SQL-based data modeling |
| **Visualization** | Streamlit (custom styling) | Interactive dashboards with sky blue theme |
| **Data Generation** | Python (Faker, pandas) | Synthetic SaaS data |
| **Admin** | pgAdmin 4 (optional) | Database inspection |

---

## Prerequisites

- **Docker Desktop** installed and running
- **Python 3.9+** for data generation, loader, and dashboard
- **dbt-core** installed (`pip install dbt-core dbt-postgres`)
- **macOS/Linux** (tested on macOS)

---

## Setup

### 1. Clone and Navigate

```bash
cd ~/Desktop/DA
```

### 2. Environment Configuration

```bash
cp .env.example .env
```

Default credentials work out of the box. Edit `.env` to customize database settings if needed.

### 3. Install Python Dependencies

```bash
# Data generator
pip install -r data_gen/requirements.txt

# Data loader
pip install -r loader/requirements.txt

# Dashboard
pip install -r app/requirements.txt
```

---

## Running the Project

### **Option 1: Full Pipeline (Recommended)**

Run everything in sequence with a single command:

```bash
make all
```

This executes:
1. `make up` â†’ Start Docker containers
2. `make gen-data` â†’ Generate 18 months of synthetic data
3. `make load-data` â†’ Load CSVs into Postgres
4. `make dbt-deps` â†’ Install dbt dependencies
5. `make dbt-run` â†’ Build all staging and mart models
6. `make dbt-test` â†’ Run data quality tests

After completion, launch the dashboard:

```bash
make app
```

The dashboard opens at **http://localhost:8501**

---

### **Option 2: Step-by-Step**

For more control, run each step individually:

```bash
# Start infrastructure
make up

# Generate synthetic data
make gen-data

# Load into warehouse
make load-data

# Run dbt transformations
make dbt-run

# Run data quality tests
make dbt-test

# Launch dashboard
make app
```

---

## Project Structure

```
/Users/saitejareddy/Desktop/DA/
â”œâ”€â”€ docker-compose.yml          # Postgres + pgAdmin containers
â”œâ”€â”€ Makefile                    # Orchestration commands
â”œâ”€â”€ .env                        # Environment configuration
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data_gen/
â”‚   â”œâ”€â”€ generator.py            # Synthetic data generation
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ output/                 # Generated CSVs (gitignored)
â”‚
â”œâ”€â”€ loader/
â”‚   â”œâ”€â”€ load_csv_to_postgres.py # CSV â†’ Postgres loader
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml            # Database connection profile
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # 9 staging models (views)
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.yml      # Schema tests
â”‚   â”‚   â”‚   â””â”€â”€ stg_*.sql
â”‚   â”‚   â””â”€â”€ marts/              # 2 dims + 7 facts (tables)
â”‚   â”‚       â”œâ”€â”€ schema.yml
â”‚   â”‚       â”œâ”€â”€ dim_*.sql
â”‚   â”‚       â””â”€â”€ fct_*.sql
â”‚   â””â”€â”€ tests/                  # Custom SQL tests
â”‚       â”œâ”€â”€ assert_non_negative_amounts.sql
â”‚       â””â”€â”€ assert_valid_subscription_dates.sql
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_app.py        # Multi-page dashboard
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ INSIGHTS.md                 # Executive insights memo
```

---

## Data Model

### **Staging Layer** (`staging` schema)
Clean, renamed, type-cast versions of raw tables:
- `stg_accounts`, `stg_users`, `stg_subscriptions`
- `stg_invoices`, `stg_payments`
- `stg_crm_deals`, `stg_product_events`
- `stg_support_tickets`, `stg_marketing_spend`

### **Marts Layer** (`marts` schema)

**Dimensions:**
- `dim_date` â€“ Calendar dimension
- `dim_account` â€“ Account attributes with current subscription status

**Facts:**
- `fct_revenue_monthly` â€“ MRR/ARR with expansion, contraction, churn, NRR
- `fct_pipeline` â€“ Deal stage conversions, win rates, sales cycle
- `fct_activation` â€“ Activation rates and time-to-activate
- `fct_retention` â€“ Cohort retention curves and churn rates
- `fct_support` â€“ Ticket volume, severity, SLA breach metrics
- `fct_cac_ltv` â€“ CAC and LTV by marketing channel (Google Ads, LinkedIn, Content Marketing, Events, Referral)
- `fct_anomalies` â€“ Month-over-month metric anomalies

---

## Dashboard

Access at **http://localhost:8501** after running `make app`.

**Features:** Modern sky blue theme with custom CSS styling for enhanced visual appeal.

### **Pages**

1. **Executive Overview**
   - KPI cards: MRR, ARR, NRR, Active Accounts, Activation Rate, Avg CAC, Churned MRR
   - MRR trend with new/expansion/churn breakdown
   - Revenue by segment pie chart

2. **Funnel & Pipeline**
   - Win rates by segment
   - Sales cycle analysis
   - Deal value distribution

3. **Retention & Cohorts**
   - Cohort retention heatmap
   - Activation vs retention correlation
   - Churn trends over time

4. **Support & Quality**
   - Ticket volume by segment and severity
   - SLA breach rates by region
   - Resolution time metrics

5. **Anomalies**
   - Automated detection of >10% MoM changes
   - Spike/drop classification
   - Metric trend visualization

### **Global Filters**
Apply segment, region, and acquisition channel (Google Ads, LinkedIn, Content Marketing, Events, Referral) filters via sidebar to slice all dashboards dynamically.

---

## Data Quality

### **Built-in dbt Tests** (20+ tests)
- Primary key uniqueness on all dimensions and facts
- Foreign key relationships between facts and dimensions
- Accepted values for categorical fields (segment, status, severity)
- Not null constraints on critical fields

### **Custom SQL Tests** (2 tests)
1. **No Negative Amounts**: Validates that invoices and MRR are never negative
2. **Valid Subscription Dates**: Ensures `start_date <= end_date` and invoices fall within subscription periods

Run all tests:
```bash
make dbt-test
```

Expected output: All tests pass

---

## Troubleshooting

### **Docker Not Running**
```bash
# Check Docker status
docker ps

# Start Docker Desktop, then:
make up
```

### **Database Connection Failed**
```bash
# Verify Postgres is healthy
docker logs saas_gtm_warehouse

# Restart containers
make down && make up
```

### **dbt Command Not Found**
```bash
pip install dbt-core dbt-postgres
```

### **No Data in Dashboard**
Ensure you've run the full pipeline:
```bash
make all
```

### **Port Conflicts**
Edit `.env` to change ports if 5432, 8080, or 8501 are in use.

---

## Cleanup

Remove generated data and dbt artifacts:
```bash
make clean
```

Stop and remove containers:
```bash
make down
```

---

## License

MIT License - Free to use for learning and portfolio purposes.

